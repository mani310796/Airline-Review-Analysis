{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import nltk.stem\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk import stem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns \n",
    "from nltk.tokenize import word_tokenize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('airline_sentiment_analysis.csv')\n",
    "main_df = main_df[['airline_sentiment','text']]\n",
    "label= main_df['airline_sentiment']\n",
    "label= list(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(label)):\n",
    "    if label[i] =='positive':\n",
    "        label[i]= 0\n",
    "    else:\n",
    "        label[i]= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(main_df['airline_sentiment'])\n",
    "main_df['Sentiment']= label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Exploration </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "0    2363\n",
       "1    9178\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt= main_df.groupby('Sentiment')\n",
    "dt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maniy\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the dataset\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot('Sentiment', data=main_df)\n",
    "plt.title('Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>From above figure we can observe that dataset is not so balanced. Though we need not do techniques like undersampling or oversampling in order to make it balanced. It is ok to proceed with this data. We will do balancing techniques if dataset is highly imbalanced i.e ex: 200:1 ratio etc</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        @VirginAmerica plus you've added commercials t...\n",
       "1        @VirginAmerica it's really aggressive to blast...\n",
       "2        @VirginAmerica and it's a really big bad thing...\n",
       "3        @VirginAmerica seriously would pay $30 a fligh...\n",
       "4        @VirginAmerica yes, nearly every time I fly VX...\n",
       "                               ...                        \n",
       "11536    @AmericanAir my flight was Cancelled Flightled...\n",
       "11537           @AmericanAir right on cue with the delays👌\n",
       "11538    @AmericanAir thank you we got on a different f...\n",
       "11539    @AmericanAir leaving over 20 minutes Late Flig...\n",
       "11540    @AmericanAir you have my money, you change my ...\n",
       "Name: text, Length: 11541, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We can observe that dataset consists of 11540 observations which are reviews(negative or positive) on particular airlines by users</h4></n><h4>Dataset needs to be processed which means, it contains special characters, smileys and alpha numeric values. So dataset needs to be processed for further proceedings of building models and classification</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pre-Processing steps</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>There are no null values in our dataset. This is the first and main preprocessing step to check whether there are any null values in our dataset.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Levelling case. In this step we will modify all the characters into same case. Generally changed to lowercase.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@virginamerica plus you've added commercials t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@virginamerica it's really aggressive to blast...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@virginamerica and it's a really big bad thing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@virginamerica seriously would pay $30 a fligh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virginamerica yes, nearly every time i fly vx...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>@americanair my flight was cancelled flightled...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>@americanair right on cue with the delays👌</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>@americanair thank you we got on a different f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>@americanair leaving over 20 minutes late flig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>@americanair you have my money, you change my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11541 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Sentiment\n",
       "0      @virginamerica plus you've added commercials t...          0\n",
       "1      @virginamerica it's really aggressive to blast...          1\n",
       "2      @virginamerica and it's a really big bad thing...          1\n",
       "3      @virginamerica seriously would pay $30 a fligh...          1\n",
       "4      @virginamerica yes, nearly every time i fly vx...          0\n",
       "...                                                  ...        ...\n",
       "11536  @americanair my flight was cancelled flightled...          1\n",
       "11537         @americanair right on cue with the delays👌          1\n",
       "11538  @americanair thank you we got on a different f...          0\n",
       "11539  @americanair leaving over 20 minutes late flig...          1\n",
       "11540  @americanair you have my money, you change my ...          1\n",
       "\n",
       "[11541 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df['text'] = [entry.lower() for entry in main_df['text']]\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleaning the text by removing bad symbols and by having only alpha numeric values</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>virginamerica plus youve added commercials to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virginamerica its really aggressive to blast ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virginamerica and its a really big bad thing ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>virginamerica seriously would pay 30 a flight...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virginamerica yes  nearly every time i fly vx...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>americanair my flight was cancelled flightled...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>americanair right on cue with the delays</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>americanair thank you we got on a different f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>americanair leaving over 20 minutes late flig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>americanair you have my money  you change my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11541 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Sentiment\n",
       "0       virginamerica plus youve added commercials to...          0\n",
       "1       virginamerica its really aggressive to blast ...          1\n",
       "2       virginamerica and its a really big bad thing ...          1\n",
       "3       virginamerica seriously would pay 30 a flight...          1\n",
       "4       virginamerica yes  nearly every time i fly vx...          0\n",
       "...                                                  ...        ...\n",
       "11536   americanair my flight was cancelled flightled...          1\n",
       "11537           americanair right on cue with the delays          1\n",
       "11538   americanair thank you we got on a different f...          0\n",
       "11539   americanair leaving over 20 minutes late flig...          1\n",
       "11540   americanair you have my money  you change my ...          1\n",
       "\n",
       "[11541 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "def Clean_It(text):\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    return text\n",
    "main_df['text'] = main_df['text'].apply(Clean_It)\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Replacing Contractions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>virginamerica plus you have added commercials...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virginamerica its really aggressive to blast ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virginamerica and its a really big bad thing ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>virginamerica seriously would pay 30 a flight...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virginamerica yes  nearly every time i fly vx...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>americanair my flight was cancelled flightled...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>americanair right on cue with the delays</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>americanair thank you we got on a different f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>americanair leaving over 20 minutes late flig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>americanair you have my money  you change my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11541 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  Sentiment\n",
       "0       virginamerica plus you have added commercials...          0\n",
       "1       virginamerica its really aggressive to blast ...          1\n",
       "2       virginamerica and its a really big bad thing ...          1\n",
       "3       virginamerica seriously would pay 30 a flight...          1\n",
       "4       virginamerica yes  nearly every time i fly vx...          0\n",
       "...                                                  ...        ...\n",
       "11536   americanair my flight was cancelled flightled...          1\n",
       "11537           americanair right on cue with the delays          1\n",
       "11538   americanair thank you we got on a different f...          0\n",
       "11539   americanair leaving over 20 minutes late flig...          1\n",
       "11540   americanair you have my money  you change my ...          1\n",
       "\n",
       "[11541 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\"youve\": \"you have\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "main_df['text']= main_df['text'].apply(replace_contractions)\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>One Small observation is that after replacing contractions, in 0th observation, youve has changed to you have which is now equivalent to 11540th observation. This step is important because we want all the wordings to be similar for further comparision and processing</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalization and Tokenization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>In this step different forms of words like organize, organizes and organizing are identified as a single word organize. Words of different character sequence is chopped into tokens which is called tokenization.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         virginamerica plus you have added commercials...\n",
       "1         virginamerica its really aggressive to blast ...\n",
       "2         virginamerica and its a really big bad thing ...\n",
       "3         virginamerica seriously would pay 30 a flight...\n",
       "4         virginamerica yes  nearly every time i fly vx...\n",
       "                               ...                        \n",
       "11536     americanair my flight was cancelled flightled...\n",
       "11537              americanair right on cue with the delay\n",
       "11538     americanair thank you we got on a different f...\n",
       "11539     americanair leaving over 20 minutes late flig...\n",
       "11540     americanair you have my money  you change my ...\n",
       "Name: text, Length: 11541, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "main_df['text']= main_df['text'].apply(lemmatize_stemming)\n",
    "main_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' virginamerica plus you have added commercials to the experience tacki'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Removing Stopwords</h1>\n",
    "<h3>Stop words are vast in number and they don't help us find the context or the true meaning of a sentence. These are words that can be removed without any negative consequences. So by removing stop words it boosts training speed.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Normalized_Reviews=[]\n",
    "for i in range(len(main_df['text'])):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(main_df['text'][i])\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:  \n",
    "        if w not in stop_words:  \n",
    "            filtered_sentence.append(w)\n",
    "    Normalized_Reviews.append(filtered_sentence)\n",
    "Normalized_Reviews= pd.Series(Normalized_Reviews)\n",
    "type(Normalized_Reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving the preprocessed data</h1><h3>We will save this data for further analysis of our models because this preprocessing may take time for different processors. By saving as csv, Next step while we are building models, we need not do preprocessing again from scratch, we just need to load the csv.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['text']= Normalized_Reviews\n",
    "main_df.to_csv('Normalized_Airlines.csv',index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Creating models for prediction and evaluation of model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df= pd.read_csv('Normalized_Airlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Splitting the data into training and testing\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(main_df['text'],main_df['Sentiment'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Normalized_Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(main_df['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "#print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions = Naive.predict(Test_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.8927952463481059\n",
      "Test Accuracy:  0.8746751371643084\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: \",Naive.score(Train_X_Tfidf,Train_Y))\n",
    "print(\"Test Accuracy: \",Naive.score(Test_X_Tfidf,Test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.39      0.55       694\n",
      "           1       0.87      1.00      0.93      2769\n",
      "\n",
      "    accuracy                           0.87      3463\n",
      "   macro avg       0.92      0.69      0.74      3463\n",
      "weighted avg       0.89      0.87      0.85      3463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Test_Y,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "SVM = LinearSVC()\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "predictions = SVM.predict(Test_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.8927952463481059\n",
      "Test Accuracy:  0.8746751371643084\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: \",Naive.score(Train_X_Tfidf,Train_Y))\n",
    "print(\"Test Accuracy: \",Naive.score(Test_X_Tfidf,Test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.71      0.77       694\n",
      "           1       0.93      0.97      0.95      2769\n",
      "\n",
      "    accuracy                           0.91      3463\n",
      "   macro avg       0.89      0.84      0.86      3463\n",
      "weighted avg       0.91      0.91      0.91      3463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Test_Y,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "Knn = KNeighborsClassifier()\n",
    "Knn.fit(Train_X_Tfidf,Train_Y)\n",
    "predictions = Knn.predict(Test_X_Tfidf)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9196583312701163\n",
      "Test Accuracy:  0.8654345942824141\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: \",Knn.score(Train_X_Tfidf,Train_Y))\n",
    "print(\"Test Accuracy: \",Knn.score(Test_X_Tfidf,Test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.71      0.68       694\n",
      "           1       0.93      0.90      0.91      2769\n",
      "\n",
      "    accuracy                           0.87      3463\n",
      "   macro avg       0.79      0.81      0.80      3463\n",
      "weighted avg       0.87      0.87      0.87      3463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Test_Y,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestClassifier(n_estimators=100,min_samples_split=25,max_depth=10)\n",
    "RF = RF.fit(Train_X_Tfidf,Train_Y)\n",
    "predictions = RF.predict(Test_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.7937608318890814\n",
      "Test Accuracy:  0.8001732601790356\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: \",RF.score(Train_X_Tfidf,Train_Y))\n",
    "print(\"Test Accuracy: \",RF.score(Test_X_Tfidf,Test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.01       694\n",
      "           1       0.80      1.00      0.89      2769\n",
      "\n",
      "    accuracy                           0.80      3463\n",
      "   macro avg       0.90      0.50      0.45      3463\n",
      "weighted avg       0.84      0.80      0.71      3463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Test_Y,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
